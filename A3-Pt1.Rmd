---
title: "Exploring Disparate Data: Part 1"
author: "15"
date: "Due November 12th"
---

```{r active="", eval=FALSE}
# BEGIN ASSIGNMENT 
```

```{r}
knitr::opts_chunk$set(error = TRUE)
# DO NOT PUT install.packages() IN AN RMD!!!
# While you're here, make sure you group number is filled in above.
    # (And make sure it ends with a quotation mark).
library(tidyverse)
theme_set(theme_bw())
library(arrow)
library(openxlsx)
```

List all group members here:

-   Alexa Melnechuk (169086699)
-   Joey Kielt (169087303)
-   Sanjana Mahabir (169080789)
-   Uma Sisira Akella (200300920)
-   Shalin Panjwani (169073107)

This file will be submitted as an Rmd file to GradeScope. You will need to create a group on GradeScope with the correct members. *If the person who submits this does not list you, you will not get a grade on this assignment!!!* Make sure you trust the person who is submitting it!

## Overview

This is **Part 1** of the group assignment, in which you'll demonstrate your ability to clean the data sets.

In this part, you will prepare the following data sets for analysis in Part 2. Note that you'll only need to use three of them, so you can focus on those three for cleaning and get started on Part 2.

Once you've completed the relevant sections, you will have the following files in your working directory:

1.  `cyclones_data.parquet` contains data for hurricane strength in both the Atlantic and North Pacific basins.
2.  `ice_extent_yearly.parquet` contains the yearly ice extent for the Arctic and the Antarctic poles.
3.  `climate_awareness.parquet` contains the proportion of people from each country who answered "no", "a little", ... to a question asking about their awareness of the actual definitions of climate change.
4.  `covid_2020.parquet` contains the total reported cases of COVID-19 in the year 2020 for all countries.
    -   It also, quite helpfully, includes the continent on which each country sits, which can be joined with other data frames (with some care).
5.  `happiness.parquet` contains information about the happiness of countries in the world, as measured by the `life_ladder` question (among other survey responses).

## NOAA data for Atlantic and Pacific Basin

### Load in the Data

In assignment 2, the NOAA data for the Atlantic was cleaned and tidied. That data will be useful for this assignment, so re-do the steps here. In this version, all of the steps can be done in a single pipeline. We'll also be doing the North Pacific (NP) basin.

```{r}
cyclone_data_address <- "https://www.nhc.noaa.gov/data/hurdat/"
at_cyclone_filename <- "hurdat2-1851-2022-050423.txt"
np_cyclone_filename <- "hurdat2-nepac-1949-2022-050423.txt"

new_columns <- c("status", "latitude", "longitude", "max_wind",
    "min_pressure", "NE_extend_34", "SE_extend_34", "SW_extend_34",
    "NW_extend_34", "NE_extend_50", "SE_extend_50", "SW_extend_50",
    "NW_extend_50", "NE_extend_64", "SE_extend_64", "SW_extend_64",
    "NW_extend_64", "r_max_wind"
)
```

```{r at_cyclone, error=TRUE}

# Same steps as in A2, but you can put them all in the same pipeline!
at_cyclone <- str_c(cyclone_data_address, at_cyclone_filename, sep = "") |>
    read_csv(
        col_names = c(as.character(1:4)),
        progress = FALSE,
        show_col_types = FALSE
    ) |>
    separate_wider_delim(
        cols = `4`,
        delim=",",
        names = new_columns
        # Set the delim and the names
        # YOUR CODE HERE
    ) |>
    mutate(
        across(everything(), str_trim),
        # make "-999" NAs, make "-99" NAs
        # Create columns BasinNumberYear, Name, and Entries
        across(everything(), ~na_if(., "-999")),
        across(everything(), ~na_if(., "-99")),
        BasinNumberYear = ifelse(is.na(status), `1`, NA),
        Name = ifelse(is.na(status), `2`, NA),
        Entries = ifelse(is.na(status), `3`, NA)
    ) |>
    relocate(BasinNumberYear, Name, Entries) |>
    fill(BasinNumberYear, Name, Entries) |>
    filter(!is.na(status))  |>
    select(-Entries) |>
    separate_wider_position(
        BasinNumberYear,
         widths=c(
      "Basin"= 2,
      "Number"= 2,
      "NameYear"= 4)
        # Specify the widths
        # YOUR CODE HERE
    ) |>
    separate_wider_position(
        `1`,
         widths=c(
      "ObservYear"= 4,
      "Month"= 2,
      "Day"= 2)
        # Specify the widths
        # YOUR CODE HERE
    ) |>
    separate_wider_position(
        `2`,
        widths = c(
          "Hour"=2, 
          "Minute"=2)
        # Specify the widths
        # YOUR CODE HERE
    ) |>
    rename(
        Identifier = `3`
    ) |>
    mutate(
        across(
            c(NameYear, ObservYear, Month, Day, Hour,
                Minute, Number),
            as.integer
        )
    ) |>
    mutate(across(max_wind:r_max_wind, as.numeric))


print(at_cyclone)
```

```{r}
. = ottr::check("tests/at_cyclone.R")
```

```{r}
np_cyclone <- str_c(cyclone_data_address, np_cyclone_filename, sep = "") |>
    read_csv(
        col_names = c(as.character(1:4)),
        progress = FALSE,
        show_col_types = FALSE
    ) |>
    separate_wider_delim(
        cols = `4`,
        delim=",",
        names = new_columns
    ) |>
    mutate(
        across(everything(), str_trim),
        across(everything(), ~na_if(., "-999")),
        across(everything(), ~na_if(., "-99")),
        BasinNumberYear = ifelse(is.na(status), `1`, NA),
        Name = ifelse(is.na(status), `2`, NA),
        Entries = ifelse(is.na(status), `3`, NA)
    ) |>
    relocate(BasinNumberYear, Name, Entries) |>
    fill(BasinNumberYear, Name, Entries) |>
    filter(!is.na(status))  |>
    select(-Entries) |>
    separate_wider_position(
        BasinNumberYear,
         widths=c(
      "Basin"= 2,
      "Number"= 2,
      "NameYear"= 4)
    ) |>
    separate_wider_position(
        `1`,
         widths=c(
      "ObservYear"= 4,
      "Month"= 2,
      "Day"= 2)
    ) |>
    separate_wider_position(
        `2`,
        widths = c(
          "Hour"=2, 
          "Minute"=2)
    ) |>
    rename(
        Identifier = `3`
    ) |>
    mutate(
        across(
            c(NameYear, ObservYear, Month, Day, Hour,
                Minute, Number),
            as.integer
        )
    ) |>
    mutate(across(max_wind:r_max_wind, as.numeric))

    # ALL of the steps all over again
    # YOUR CODE HERE

print(np_cyclone)
```

```{r}
. = ottr::check("tests/np_cyclone.R")
```
### Combine into One Data Frame

Now, bind the two data frames together (row-wise). Since the `Basin` column already contains the information about which basin the cyclone was in, there's no need to specify an ID. (Hint: don't overthink this one.)

Name the data frame `cyclones_data_update_0`.

```{r error=TRUE}
cyclones_data_update_0 <- bind_rows(at_cyclone, np_cyclone)

print(cyclones_data_update_0)
```
```{r}
. = ottr::check("tests/cyclones_data_update_1.R")
```
### Fix Latitude and Longitude

The latitude and longitude are not in a format that makes for nice plotting. They're encoded as "`28.0N`" for 28 degrees North and "28.0S" for 28 degrees South of the equator. By convention, "`28.0N`" should be `28` degrees (a positive number) and `28.8S` should be -28 degrees (a negative number). Similarly, `94W` should be -94 while 94E should be 94. Fix this.

Write a function that takes in a vector of latitude *or* longitude in the format of these data, then outputs a positive version of the numeric part if there's an "N" or an "E", and a negative version of the numeric part if there's a "W" or "S".

There are many ways to solve this. My solution is one line and uses `parse_number()` and a single `if_else()` statement with a regex, but there are many possible ways to do this!

```{r convert_latlon, error=TRUE}
convert_latlon <- function(latlon) {
    if_else(str_detect(latlon,"[SW]"),true=-parse_number(latlon), false =parse_number(latlon))
}

test_data <- c("49W", "49.99W", "49E", "49.99E", "49N", "49.99S", "-0.0W")

# If this returns "TRUE", then your function will work on the real data.
all.equal(
    convert_latlon(test_data),
    c(-49, -49.99, 49, 49.99, 49, -49.99, 0)
)

cyclones_data_update_1 <- cyclones_data_update_0 |>
    mutate(
        lat = convert_latlon(latitude),
        lon = convert_latlon(longitude)
    )
cyclones_data_update_1
```
```{r}
. = ottr::check("tests/convert_latlon.R")
```
### Extract Dates

We have a few more tasks. First, create a [datetime](https://fralfaro.github.io/r4ds/datetimes.html) object using the `make_datetime()` function so that R knows how to plot the times. Label the new column as "`date`".

To do this, you must choose whether to use `ObservYear` or `NameYear`.

```{r cyclones_data_update_2, error=TRUE}
cyclones_data_update_2 <- cyclones_data_update_1 |>
    mutate(date=make_datetime(ObservYear, Month, Day, Hour, Minute)
        # YOUR CODE HERE
    )

print(cyclones_data_update_2)
```
```{r}
. = ottr::check("tests/cyclones_data_update_2.R")
```
### Assign Storm Categories

Unlike in assignment 1, the category is not part of these data. However, we learned how storms are categorized! In particular, meteorologists use the [Saffir--Simpson hurricane wind scale (SSHWS)](https://en.wikipedia.org/wiki/Saffir%E2%80%93Simpson_scale) (note that our data are measured in knots on that scale).

Categorize the cyclones based on the scale above using a `case_when()` statement based on the `max_wind` column. The code is set up to create an [*ordered factor*](https://fralfaro.github.io/r4ds/factors.html) so that R knows that the levels are ordered as TD \< TS \< 1 \< 2 \< 3 \< 4 \< 5.

```{r cyclones_data}
cat_levels <- c("TD", "TS", "1", "2", "3", "4", "5")

cyclones_data <- cyclones_data_update_2 |>
    mutate(
        category = ordered(
            case_when(
              max_wind<=33~"TD", 
              max_wind<=63~"TS",
              max_wind<=82~"1",
              max_wind<=95~"2", 
              max_wind<=112~"3", 
              max_wind<=136~"4",
              max_wind>=137~"5"
                # YOUR CODE HERE
            ),
            levels = cat_levels
        )
    )

print(cyclones_data)
```
```{r}
. = ottr::check("tests/cyclones_update_2.R")
```
### Plots and Summary Statistics!

### Clean Up

Now that we have a suitable data frame, save it as a parquet file with the file name `"cyclones_data.parquet"`.

```{r}
write_parquet(cyclones_data, "cyclones_data.parquet")
```

```{r}
. = ottr::check("tests/write_cyclones.R")
```
## Sea Ice Data

The following data are used in the Course Notes to demonstrate reading in specific sheets. In this assignment, we care most about what's *in* the data! I've copied the code over from the notes; there are no questions on loading the data.

The sea ice extent is [defined](https://arctic.noaa.gov/report-card/report-card-2023/sea-ice-2023/) as the total area of the ocean that is covered in ice of at least 15% concentration.

```{r}
sea_ice_extent_xlsx <- "https://masie_web.apps.nsidc.org/pub//DATASETS/NOAA/G02135/seaice_analysis/Sea_Ice_Index_Daily_Extent_G02135_v3.0.xlsx"
```

```{r}
NH_daily <- sea_ice_extent_xlsx |>
    read.xlsx(
        sheet = "NH-Daily-Extent",
    ) |>
    select(X1, X2, `1978`:`2023`) |>
    rename(
        month = X1,
        day = X2
    ) |>
    fill(month) |>
    pivot_longer(
        cols = `1978`:`2023`,
        names_to = "year",
        values_to = "ice_extent",
        values_drop_na = TRUE,
    ) |>
    mutate(
        year = as.integer(year),
        month = ordered(
            month,
            levels = c("January", "February", "March", "April",
                "May", "June", "July", "August", "September",
                "October", "November", "December")),
        region = "Arctic",
    ) |>
    arrange(
        year, month, day
    )
```

```{r}
SH_daily <- sea_ice_extent_xlsx |>
    read.xlsx(
        sheet = "SH-Daily-Extent",
        skipEmptyCols = TRUE,
        fillMergedCells = TRUE,
        cols = 1:48
    ) |>
    rename(
        month = X1,
        day = X2
    ) |>
    pivot_longer(
        cols = `1978`:`2023`,
        names_to = "year",
        names_transform = list(year = as.integer),
        values_to = "ice_extent",
        values_drop_na = TRUE,
    ) |>
    mutate(
        month = ordered(
            month,
            levels = c("January", "February", "March", "April",
                "May", "June", "July", "August", "September",
                "October", "November", "December")
        ),
        region = "Antarctic",
    ) |>
    arrange(
        year, month, day
    )
```

```{r}
ice_extent_daily <- bind_rows(NH_daily, SH_daily) |>
    mutate(date = make_date(year, month, day)) |>
    arrange(region, date)

ice_extent_daily
```

## Stats and Plots

```{r}
ice_extent_daily |>
    ggplot() +
        aes(x = yday(date), y = ice_extent, colour = year, group = factor(year)) +
        geom_line() +
        facet_wrap(~region) +
        #coord_polar() +
        scale_colour_distiller(
            direction = 1, type = "seq", palette = 3
        )
```
In the chunk below, find the maximum and minimum sea ice extent for each year (by region). Pivot the data frame so that there's a column labelled "name" and a column labelled "value", as below.

| year | region    | name | value  |
|------|-----------|------|--------|
| 1978 | Antarctic | min  | 7.283  |
| 1978 | Antarctic | max  | 17.803 |
| 1978 | Arctic    | min  | 10.231 |
| 1978 | Arctic    | max  | 14.585 |
| 1979 | Antarctic | min  | 2.911  |
| 1979 | Antarctic | max  | 18.361 |
| ...  | ...       | ...  | ...    |

```{r}
ice_extent_yearly <- ice_extent_daily |>
  group_by(year, region)|>
  summarise( max = max(ice_extent, na.rm=TRUE), min = min(ice_extent, na.rm=TRUE))|>
  pivot_longer(cols=c(min,max),
    names_to = "name", 
    values_to = "value")|> as_tibble()

ice_extent_yearly
```

```{r}
. = ottr::check("tests/ice_extent_yearly.R")
```

Given your answer, the following code should produce a plot thats easier to interpret than the one before.

```{r plot_sea_ice_extent}
ggplot(ice_extent_yearly) +
    aes(x = year, y = value, colour = name) +
    geom_line() +
    facet_wrap(~ region) +
    labs(
        x = "Year", y = "Sea Ice Extent",
        colour = "Stat",
        title = "Min and Max Sea Ice Extent, by Year",
        subtitle = "Arctic is clearly decreasing, Antarctic is possibly becoming more variable."
    )
```

Finally, let's save the data to `ice_extent_yeary.parquet`.

```{r sea_ice_parquet, error=TRUE}
write_parquet(ice_extent_yearly, "ice_extent_yearly.parquet")
```

```{r}
. = ottr::check("tests/sea_ice_parquet.R")
```
